apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-apiserver
  namespace: kube-system
  labels:
    tier: control-plane
    k8s-app: kube-apiserver
    {{- if eq .Values.replicas 1.0 }}
    pdb: controlplane
    {{- end }}
spec:
  replicas: {{ .Values.replicas }}
  selector:
    matchLabels:
      tier: control-plane
      k8s-app: kube-apiserver
      {{- if eq .Values.replicas 1.0 }}
      pdb: controlplane
      {{- end }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      # If only one instance is running, let it be unavailable, as we use
      # host port, so we cannot have 2 instances running at the same time on a
      # single node.
      #
      # If there is only one replica of the pod, don't allow it to be unavailable.
      # However, we use hostPort for exposing it, so updating it won't work, as it
      {{- if eq .Values.replicas 1.0 }}
      maxUnavailable: 0
      {{- else }}
      maxUnavailable: 1
      {{- end }}
  template:
    metadata:
      labels:
        tier: control-plane
        k8s-app: kube-apiserver
        {{- if eq .Values.replicas 1.0 }}
        pdb: controlplane
        {{- end }}
      annotations:
        checkpointer.alpha.coreos.com/checkpoint: "true"
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      #hostNetwork: true
      #nodeSelector:
      #  node-role.kubernetes.io/master: ""
      priorityClassName: system-cluster-critical
      serviceAccountName: kube-apiserver
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: kube-apiserver
        image: k8s.gcr.io/hyperkube:v{{ .Chart.AppVersion }}
        command:
        - /hyperkube
        - kube-apiserver
        - --etcd-servers={{ .Values.etcdServers }}
        - --bind-address=0.0.0.0
        - --client-ca-file=/etc/kubernetes/pki/ca.crt
        - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
        - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
        # Required for TLS bootstrapping
        - --enable-bootstrap-token-auth=true
        # Override default service cluster IP, as it conflicts with host CIDR.
        - --service-cluster-ip-range={{ .Values.serviceCIDR }}
        # To disable access without authentication
        - --insecure-port=0
        # Since we will run self-hosted K8s, pods like kube-proxy must run as privileged containers, so we must allow them.
        - --allow-privileged=true
        - --authorization-mode=RBAC,Node
        # Required to validate service account tokens created by controller manager
        - --service-account-key-file=/etc/kubernetes/pki/service-account.crt
        # IP address which will be added to the kubernetes.default service endpoint
        #- --advertise-address=10.0.0.2
        # Bootstrap control plane is running on port 8443 instead of standard 6443, as haproxy is listening on 6443 for
        # load balancing and failover. Boostrap kube-apiserver registers kubernetes.default.svc with port 8443, so we need to
        # use the same port here as bootstrap kube-apiserver.
        - --secure-port=8443
        # Prefer to talk to kubelets over InternalIP rather than via Hostname or DNS, to make it more robust
        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        volumeMounts:
        - name: secrets
          mountPath: /etc/kubernetes/pki
          readOnly: true
        - name: ssl-certs-host
          mountPath: /etc/ssl/certs
          readOnly: true
        ports:
        - containerPort: 8443
      volumes:
      - name: secrets
        secret:
          secretName: kube-apiserver
      - name: ssl-certs-host
        hostPath:
          path: /etc/ssl/certs
