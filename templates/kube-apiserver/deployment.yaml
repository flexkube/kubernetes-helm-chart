apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-apiserver
  namespace: kube-system
  labels:
    tier: control-plane
    k8s-app: kube-apiserver
    {{- if eq .Values.replicas 1.0 }}
    pdb: controlplane
    {{- end }}
spec:
  replicas: {{ .Values.replicas }}
  selector:
    matchLabels:
      tier: control-plane
      k8s-app: kube-apiserver
      {{- if eq .Values.replicas 1.0 }}
      pdb: controlplane
      {{- end }}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      # If only one instance is running, let it be unavailable, as we use
      # host port, so we cannot have 2 instances running at the same time on a
      # single node.
      #
      # If there is only one replica of the pod, don't allow it to be unavailable.
      # However, we use hostPort for exposing it, so updating it won't work, as it
      {{- if eq .Values.replicas 1.0 }}
      maxUnavailable: 0
      {{- else }}
      maxUnavailable: 1
      {{- end }}
  template:
    metadata:
      labels:
        tier: control-plane
        k8s-app: kube-apiserver
        {{- if eq .Values.replicas 1.0 }}
        pdb: controlplane
        {{- end }}
      annotations:
        checkpointer.alpha.coreos.com/checkpoint: "true"
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: tier
                  operator: In
                  values:
                  - control-plane
                - key: k8s-app
                  operator: In
                  values:
                  - kube-apiserver
              topologyKey: kubernetes.io/hostname
      # Host network is required to be able to communicate with etcd, which runs outside of the cluster.
      # When running single control plane node, this is not really needed, but it simplifies the config
      # and makes updating slightly better, as the alternative is to use hostPort, which prevents pod from being
      # scheduled if previous pod still occupies the node. This make update rely on kube-scheduler without kube-apiserver
      # running.
      # With `hostNetwork:true` pod is scheduled on the node, but it will be in crash loop until previous one is manually
      # removed, this is OK.
      hostNetwork: true
      #nodeSelector:
      #  node-role.kubernetes.io/master: ""
      priorityClassName: system-cluster-critical
      serviceAccountName: kube-apiserver
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: kube-apiserver
        image: k8s.gcr.io/hyperkube:v{{ .Chart.AppVersion }}
        command:
        - /hyperkube
        - kube-apiserver
        - --etcd-servers={{ .Values.etcdServers }}
        # We run on host network, so we bind only on host IP, which ideally would not be a public interface.
        - --bind-address=$(HOST_IP)
        # We run on host network, but we want to advertise Pod IP address to kubernetes.default.svc service endpoint.
        # Without this line, we may get public IP address being added to the endpoint, which is not correct.
        - --advertise-address=$(HOST_IP)
        # CA certificate for validating API clients
        - --client-ca-file=/etc/kubernetes/pki/ca.crt
        # TLS certificates for HTTPS serving
        - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
        - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
        # Required for TLS bootstrapping
        - --enable-bootstrap-token-auth=true
        # Override default service cluster IP, as it conflicts with host CIDR.
        - --service-cluster-ip-range={{ .Values.serviceCIDR }}
        # To disable access without authentication
        - --insecure-port=0
        # Since we will run self-hosted K8s, pods like kube-proxy must run as privileged containers, so we must allow them.
        - --allow-privileged=true
        - --authorization-mode=RBAC,Node
        # Required to validate service account tokens created by controller manager
        - --service-account-key-file=/etc/kubernetes/pki/service-account.crt
        # Bootstrap control plane is running on port 8443 instead of standard 6443, as haproxy is listening on 6443 for
        # load balancing and failover. Boostrap kube-apiserver registers kubernetes.default.svc with port 8443, so we need to
        # use the same port here as bootstrap kube-apiserver.
        - --secure-port=8443
        # Prefer to talk to kubelets over InternalIP rather than via Hostname or DNS, to make it more robust
        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
        livenessProbe:
          httpGet:
            scheme: HTTPS
            path: /livez
            port: 8443
          initialDelaySeconds: 30
          timeoutSeconds: 30
        env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        volumeMounts:
        - name: secrets
          mountPath: /etc/kubernetes/pki
          readOnly: true
      volumes:
      - name: secrets
        secret:
          secretName: kube-apiserver
